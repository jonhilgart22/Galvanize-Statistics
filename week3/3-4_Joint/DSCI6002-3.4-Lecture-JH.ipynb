{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3.4: Jointly Distributed Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* The joint distribution function\n",
    "* Marginal distributions\n",
    "* Independent random variables\n",
    "* Conditional distributions\n",
    "* Conditional expectation\n",
    "* Combining random variables\n",
    "    * Covariance\n",
    "    * Correlation\n",
    "    * Mean\n",
    "    * Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Relationship Between Two Random Variables\n",
    "\n",
    "<img src=\"images/relationship.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Previously, we talked about the distribution, mean and variance for a single random variable.\n",
    "\n",
    "* For today's class we will mainly focus on the relationship between two *discrete* random variables  \n",
    "\n",
    "* The same rules hold for *continuous* random variables - refer to [this link](http://www.colorado.edu/economics/morey/6818/jointdensity.pdf) for a detailed discussion on the continuous case along with examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Joint Distribution Function\n",
    "\n",
    "* When we deal with two discrete random variables, $X$ and $Y$, it is convenient to work with joint probabilities. We define the joint probability distribution to be    \n",
    "\n",
    "\n",
    "$$ f_{X, Y} (x, y) = P(X = x \\text{ and } Y = y) $$\n",
    "\n",
    "* As usual, we require that  \n",
    "\n",
    "\n",
    "$$f_{X, Y} (x, y) \\geq 0 \\text{ for any pairs } x, y$$  \n",
    "    \n",
    "$$\\sum_{\\text{all } x, y} f_{X, Y} (x, y) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 1\n",
    "\n",
    "Students in a college were classified according to years in school ($X$) and number of visits to a museum in the last year (0 for no visits, 1 for one visit, 2 for more than one visit). The joint probabilities in the accompanying table were estimated for these random variables.\n",
    "\n",
    "<img src=\"images/joint_example1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example 2\n",
    "\n",
    "The accompanying table shows, for credit card holders with one to three cards, the joint probabilities for number of cards owned ($X$) and number of credit purchases made in a week ($Y$).\n",
    "\n",
    "<img src=\"images/joint_example2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Marginal Distributions\n",
    "\n",
    "* Suppose we have the joint distribution of $X$ and $Y$, but we are only interested in $X$. We can obtain the marginal distribution of $X$ as follows, \n",
    "\n",
    "    $$ f_X(x) = \\sum_y f_{X, Y} (x, y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The marginal probabilities of $Y$ are given by  \n",
    "    \n",
    "    $$ f_Y(y) = \\sum_x f_{X, Y} (x, y) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The term “marginal” merely describes how the distribution of X can be calculated from the joint distribution of X and another variable Y; row sums (or column sums) are calculated and placed “in the margin” of the probability table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Compute the Marginal Distributions\n",
    "\n",
    "<img src=\"images/joint_example1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|  x  | P(X = x) |\n",
    "|:---:|:--------:|\n",
    "|  1  |          |\n",
    "|  2  |          |\n",
    "|  3  |          |\n",
    "|  4  |          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|  y  | P(Y = y) |\n",
    "|:---:|:--------:|\n",
    "|  1  |          |\n",
    "|  2  |          |\n",
    "|  3  |          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|  x  | P(X = x) |\n",
    "|:---:|:--------:|\n",
    "|  1  |   0.24   |\n",
    "|  2  |   0.20   |\n",
    "|  3  |   0.29   |\n",
    "|  4  |   0.27   |\n",
    "\n",
    "\n",
    "|  y  | P(Y = y) |\n",
    "|:---:|:--------:|\n",
    "|  1  |   0.17   |\n",
    "|  2  |   0.56   |\n",
    "|  3  |   0.27   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Independence\n",
    "\n",
    "* Two random variables $X$ and $Y$ are called independent if the events ($X = x$) and ($Y = y$) are independent. That is,\n",
    "\n",
    "* The random variables $X$ and $Y$ are independent if for all values of x and y:\n",
    "\n",
    "$$ f_{X, Y} (x, y) = f_X (x) f_Y(y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"images/joint_example2.png\" width=\"500\">\n",
    "\n",
    "* What is the $P(X = 2 \\text{ and } Y = 3)$?  \n",
    "\n",
    "* Calculate the marginal probabilities.  \n",
    "\n",
    "* Are $X$ and $Y$ independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Distributions\n",
    "\n",
    "* Let $X$ and $Y$ be jointly distributed random variables. Then the conditional distribution of $X$ given $Y$ is given by\n",
    "\n",
    "$$ f_{X|Y} (x | y) = \\frac{f_{X, Y} (x, y)}{f_Y (y)} $$\n",
    "\n",
    "* Note that for a given $y$ value, $f_{X|Y} (x | Y = y)$ is a probability distribution. That is, for any value $y$\n",
    "\n",
    "$$ \\sum_{\\text{all } x \\text{ values}} P(X = x | Y = y) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "Does money make you happy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/money_happy.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Given the fact that you're very happy (i.e. $Y = 2$), what is the conditional distribution of your salary?\n",
    "\n",
    "* That is, we want to compute $f_{X|Y} (x | Y = 2)$ (or $P(X = x | Y = 2)$)  \n",
    "\n",
    "\n",
    "|  $x$ | P(X = x $|$ Y = 2) |\n",
    "|:----:|:------------------:|\n",
    "|  2.5 |  0.07/0.46 = 0.15  |\n",
    "|  7.5 |                    |\n",
    "| 12.5 |                    |\n",
    "| 17.5 |                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "|  $x$ | P(X = x $|$ Y = 2) |\n",
    "|:----:|:------------------:|\n",
    "|  2.5 |  0.07/0.46 = 0.15  |\n",
    "|  7.5 |  0.11/0.46 = 0.24  |\n",
    "| 12.5 |  0.14/0.46 = 0.30  |\n",
    "| 17.5 |  0.14/0.26 = 0.30  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note: these probabilities do not exactly add up to 1 due to the rounding of the decimals. If we had included more decimal places, they should add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Expectation\n",
    "\n",
    "* One useful application of conditional distributions is in calculating conditional expectations. You will see a lot more of this when we get to regression analysis.\n",
    "\n",
    "* A conditional distribution is simply a probablity distribution, therefore, we could find the expectation of this distribution by:\n",
    "\n",
    "$$ E(X | Y = y) = \\sum_{\\text{all } x \\text{ values}} x \\text{ } f_{X|Y} (x | Y = y) = \\sum_{\\text{all } x \\text{ values}} x \\text{ } P(X = x | Y = y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"images/money_happy.png\" width=\"400\">\n",
    "\n",
    "* What is the expected salary for someone who is depressed?\n",
    "\n",
    "* We need to compute $E(X | Y = 0)$. How do we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|  $x$ | P(X = x $|$ Y = 0) |\n",
    "|:----:|:------------------:|\n",
    "|  2.5 |  0.03/0.07 = 0.43  |\n",
    "|  7.5 |  0.02/0.07 = 0.29  |\n",
    "| 12.5 |  0.01/0.07 = 0.14  |\n",
    "| 17.5 |  0.01/0.07 = 0.14  |\n",
    "\n",
    "\n",
    "\n",
    "$$ \\begin{align*} \n",
    "     E(X | Y = 0) &= \\sum x_i P(X = x_i | Y = 0) \\\\\n",
    "                  &= 2.5(0.43) + 7.5(0.29) + 12.5(0.14) + 17.5(0.14) \\\\\n",
    "                  &= 7.45\n",
    "   \\end{align*}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combining Random Variables\n",
    "\n",
    "To understand how to calculate $E(X + Y)$ and $Var(X + Y)$, we need to first introduce the concepts of covariance and correlation for random variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Covariance\n",
    "\n",
    "* The variance of a random variable is a measure of its variability, and the covariance of two random variables is a measure of their joint variability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The covariance is a measure of the linear association of two random variables. Its sign reflects the direction of the association; if the variables tend to move in the same direction the covariance is positive. If the variables tend to move in opposite directions the covariance is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The covariance is a pain to calculate, but for completeness, here is the theoretical formula,\n",
    "\n",
    "$$ \\begin{align}\n",
    "Cov(X, Y) &= \\sigma_{XY} \\\\\n",
    "&= E[(X - E(X)(Y - E(Y))] \\\\\n",
    "&= \\sum_{i = 1}^N (x_i - E(X))(y_i - E(Y))P(x_i, y_i)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The covariance can also be expressed as\n",
    "\n",
    "$$ Cov(X, Y) = E(XY) - E(X)E(Y) $$\n",
    "\n",
    "* Two interesting facts\n",
    "    * $Cov(X, X) = Var(X)$\n",
    "    \n",
    "    * if $X$ and $Y$ are **independent**, $Cov(X, Y) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Example\n",
    "\n",
    "<img src=\"images/stock_bond.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$ \\begin{align*} \n",
    "     E(S) &= -10(0.10) + 0(0.20) + 10(0.40) + 20(0.30) \\\\\n",
    "          &= 9 \\\\\n",
    "     E(T) &= 6(0.20) + 8(0.60) + 10(0.20) \\\\\n",
    "          &= 8 \\\\\n",
    "   \\end{align*}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "   $$ \\begin{align*} \n",
    "   Cov(S, T) &= (-10 - 9)(6 - 8)(0) \\\\\n",
    "               &  + (0 - 9)(6 - 8)(0) \\\\\n",
    "               &  + (10 - 9)(6 - 8)(0.10) \\\\\n",
    "               &  + (20 - 9)(6 - 8)(0.10) \\\\\n",
    "               &  + (-10 - 9)(8 - 8)(0) \\\\\n",
    "               &  + (0 - 9)(8 - 8)(0.10) \\\\\n",
    "               &  + (10 - 9)(8 - 8)(0.30) \\\\\n",
    "               &  + (20 - 9)(8 - 8)(0.20) \\\\\n",
    "               &  + (-10 - 9)(10 - 8)(0.10) \\\\\n",
    "               &  + (0 - 9)(10 - 8)(0.10) \\\\\n",
    "               &  + (10 - 9)(10 - 8)(0) \\\\\n",
    "               &  + (20 - 9)(10 - 8)(0) \\\\\n",
    "               &= -9.1\n",
    "   \\end{align*}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Covariance Matrix\n",
    "\n",
    "Sometimes the covariance between random variables is presented in a table, or matrix of the following form:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "    Var(X_1) & Cov(X_1, X_2) & Cov(X_1, X_3) \\\\\n",
    "    Cov(X_2, X_1) & Var(X_2) & Cov(X_2, X_3) \\\\\n",
    "    Cov(X_3, X_1) & Cov(X_3, X_2) & Var(X_3) \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "This is called a covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Covariance and Independence\n",
    "\n",
    "* IF $X$ and $Y$ are independent, then $Cov(X, Y) = 0$.\n",
    "* But the reverse is not always true!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**: \n",
    "\n",
    "* Suppose $X$ is a random variable with  \n",
    "\n",
    "\n",
    "$$ P(X = -1) = P(X = 0) = P(X = 1) = 1/3 $$\n",
    "\n",
    "* Then\n",
    "\n",
    "$$ E(X) = -1(1/3) + 0(1/3) + 1(1/3) = 0 $$\n",
    "$$ Var(X)= (-1 - 0)^2 (1/3) + (0 - 0)^2 (1/3) + (1 - 0)^2 (1/3) = 2/3$$\n",
    "\n",
    "* Let   \n",
    "\n",
    "$$Y = 1 - X^2$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Note $XY = 0$&nbsp; always  \n",
    "\n",
    "* So   \n",
    "\n",
    "$$Cov(X,Y) = E(XY) - E(X)E(Y) = 0$$\n",
    "\n",
    "* But $Y$ is a function of $X$ so they not independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlation: Covariance Rescaled\n",
    "\n",
    "* Covariance can indicate whether $X$ and $Y$ have a positive, negative, or zero relation. But it is not a great measure of association since it depends on the units of measurement.\n",
    "\n",
    "* To eliminate this difficulty, we define the correlation:\n",
    "$$ \\rho = \\frac{\\sigma_{X, Y}}{\\sigma_X \\sigma_Y} $$\n",
    "\n",
    "* This is a unitless measure of association.\n",
    "\n",
    "* The correlation is always between -1 and 1, with 1 indicating a perfect positive linear relationship, -1 a perfect negative linear relationship and 0 no linear relationship between $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Combination of Random Variables\n",
    "\n",
    "* If $X$ and $Y$ are independent  \n",
    "\n",
    "$$ E(X + Y) = E(X) + E(Y) $$  \n",
    "\n",
    "$$ Var(X + Y) = Var(X) + Var(Y) $$  \n",
    "\n",
    "* If $X$ and $Y$ are not independent  \n",
    "\n",
    "$$ E(X + Y) = E(X) + E(Y) $$  \n",
    "\n",
    "$$ Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y) $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In more general cases  \n",
    "\n",
    "$$ E((a + bX) + (c + dY)) = a + bE(X) + c + dE(Y) $$  \n",
    "\n",
    "$$ Var((a + bX) + (c + dY)) = b^2Var(X) + d^2Var(Y) + 2bdCov(X, Y) $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Investigating the Sample Mean\n",
    "\n",
    "* Consider the mean of $n$ independent and identically distributed (i.i.d.) random variables with mean $\\mu$ and variance $\\sigma^2$\n",
    "\n",
    "$$ \\bar{X} = \\frac{1}{n} \\sum_{i = 1}^n X_i $$\n",
    "\n",
    "\n",
    "* Calculate the mean of the mean, $E(\\bar{X})$\n",
    "\n",
    "* Calculate the variance of the mean, $Var(\\bar{X})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\begin{align*} \n",
    "     E(\\bar{X}) &= E(\\frac{1}{n} \\sum_{i = 1}^n X_i) \\\\\n",
    "                &= \\frac{1}{n} E(\\sum_{i = 1}^n X_i) \\\\\n",
    "                &= \\frac{1}{n} \\sum_{i = 1}^n E(X_i) \\\\\n",
    "                &= \\frac{1}{n} \\sum_{i = 1}^n \\mu \\\\\n",
    "                &= \\mu\n",
    "   \\end{align*}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\begin{align*} \n",
    "     Var(\\bar{X}) &= Var(\\frac{1}{n} \\sum_{i = 1}^n X_i) \\\\\n",
    "                  &= \\frac{1}{n^2} Var(\\sum_{i = 1}^n X_i) \\\\\n",
    "                  &= \\frac{1}{n^2} \\sum_{i = 1}^n Var(X_i) \\\\\n",
    "                  &= \\frac{1}{n^2} \\sum_{i = 1}^n \\sigma^2 \\\\\n",
    "                  &= \\frac{\\sigma^2}{n}\n",
    "   \\end{align*}  $$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
