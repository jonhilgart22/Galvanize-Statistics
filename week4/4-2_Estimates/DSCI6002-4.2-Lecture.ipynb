{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 4.2: Point Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "* Difference between estimand, estimator and estimate\n",
    "\n",
    "* Methods of finding estimators\n",
    "    * Method of moments (MOM)\n",
    "    * Maximum likelihood estimation (MLE)\n",
    "* Evaluating estimators\n",
    "    * Mean squared error (MSE)\n",
    "        * Bias of an estimator\n",
    "        * Variance of an estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "* Understand clearly the difference between an estimator and an estimate\n",
    "* Know how to use MOM for estimation\n",
    "* Know how to find the MLE for a parameter of interest\n",
    "* Know how to evaluate an estimator using mean squared error (MSE)\n",
    "* Have a general understanding of bias-variance trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimand, Estimator and Estimate\n",
    "\n",
    "* **Estimand**: what we are trying to estimate (the quantity of interest)\n",
    "    * e.g. the population mean, $\\mu$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Estimator**: what we use to estimate the estimand (the rule)\n",
    "    * e.g. the sample mean $\\bar{X} = \\frac{1}{n} \\sum_{i = 1}^n X_i$\n",
    "    * the estimator contains random variables, not the actual observed data for them\n",
    "    * the estimator itself is a random variable\n",
    "    * the estimator has its mean and variance  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* **Estimate**: the actual estimate given by the estimator and a set of data (the result)\n",
    "    * e.g. the sample mean we calculate from real data $\\bar{x} = \\frac{1}{n} \\sum_{i = 1}^n x_i$\n",
    "    * an estimate is realization of the estimator\n",
    "    * estimator vs estimate is just like $X$ vs $x$ (the random variable vs one value of the random variable)\n",
    "    * the estimate is a fixed quantity   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods of Finding Estimators\n",
    "\n",
    "<img src=\"images/coins.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method of Moments (MOM)\n",
    "\n",
    "Coin experiment: 8 heads out of 10 tosses  \n",
    "\n",
    "We wanted to estimate the true probability of getting a head (the estimand) \n",
    "\n",
    "You said: it's common sense, $8/10 = 0.8$ (an estimate based on the data observed) \n",
    "\n",
    "You were essentially using a moment estimator (method of moments)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**What are moments?**\n",
    "\n",
    "The $k^{th}$ **moment** of a random variable $X$ is defined to be $E(X^k)$ assuming that $E(|X|^k) < \\infty$.\n",
    "\n",
    "e.g.\n",
    "\n",
    "* The $1^{st}$ moment of $X$: $E(X)$\n",
    "* The $2^{nd}$ moment of $X$: $E(X^2)$\n",
    "* The $3^{rd}$ moment of $X$: $E(X^3)$\n",
    "* $\\dots$\n",
    "\n",
    "Note:\n",
    "\n",
    "$$\\mu = E(X)$$  \n",
    "\n",
    "\n",
    "$$\\sigma^2 = E(X^2) - (E(X))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Moment estimators**\n",
    "\n",
    "* The **method of moments** is probably the oldest method of finding point estimators, and the estimators found are called **moment estimators**.  \n",
    "\n",
    "\n",
    "* Suppose we have a sample, $X_1, \\dots, X_n$ from a population with PDF or PMF $f(x | \\theta_1, \\dots, \\theta_k)$, and we want to estimate the $k$ parameters of the population (the $\\theta_i$'s).  \n",
    "\n",
    "\n",
    "* Method of moments estimators are found by equating the first $k$ sample moments to the corresponding $k$ population moments, and solve the system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$X_1, \\dots, X_n \\sim f(x | \\theta_1, \\dots, \\theta_k)$  \n",
    "\n",
    "* Sample moments:\n",
    "$$ m_1 = \\frac{1}{n} \\sum_{i = 1}^n X_i $$\n",
    "$$ m_2 = \\frac{1}{n} \\sum_{i = 1}^n X_i^2 $$\n",
    "$$ \\vdots $$\n",
    "$$ m_k = \\frac{1}{n} \\sum_{i = 1}^n X_i^k $$\n",
    "$$ \\vdots $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Population moments:  \n",
    "$$ \\mu_1^{'} = E(X) $$\n",
    "$$ \\mu_2^{'} = E(X^2) $$\n",
    "$$ \\vdots $$\n",
    "$$ \\mu_k^{'} = E(X^k) $$\n",
    "$$ \\vdots $$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Let the sample moments be the estimates of the population moments:  \n",
    "$$ \\frac{1}{n} \\sum_{i = 1}^n X_i = \\hat{E(X)} $$\n",
    "$$ \\frac{1}{n} \\sum_{i = 1}^n X_i^2 = \\hat{E(X^2)} $$\n",
    "$$ \\vdots $$\n",
    "$$ \\frac{1}{n} \\sum_{i = 1}^n X_i^k = \\hat{E(X^k)} $$\n",
    "$$ \\vdots $$  \n",
    "\n",
    "\n",
    "* Solve the system of equations for $\\theta_i$'s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: Normal method of moments**\n",
    "\n",
    "* Suppose $X_1, \\dots, X_n$ are i.i.d. (independently, identically distributed) $N(\\mu, \\sigma^2)$. In the preceding notation, $\\theta_1 = \\mu$ and $\\theta_2 = \\sigma^2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We have \n",
    "\n",
    "$$m_1 = \\bar{X} \\text{, } m_2 = \\frac{1}{n} \\sum X_i^2$$  \n",
    "\n",
    "\n",
    "$$E(X) = \\mu$$  \n",
    "\n",
    "\n",
    "$$E(X^2) = \\mu^2 + \\sigma^2 $$  \n",
    "$$(Var(X) = \\sigma^2 = E(X^2) - (E(X))^2 = E(X^2) - \\mu^2) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Equating the sample moments to the estimates of the population moments\n",
    "$$ \\bar{X} = \\hat{\\mu}$$\n",
    "$$ \\frac{1}{n} \\sum X_i^2 = \\hat{\\mu}^2 + \\hat{\\sigma}^2 $$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Solve for $\\mu$ and $\\sigma^2$, we get:  \n",
    "\n",
    "$$ \\hat{\\mu} = \\bar{X} $$  \n",
    "\n",
    "\n",
    "$$ \\hat{\\sigma}^2 = \\frac{1}{n} \\sum X_i^2 - \\bar{X}^2 = \\frac{1}{n} \\sum (X_i - \\bar{X})^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Advantages of MOM**\n",
    "\n",
    "* Simple to calculate\n",
    "* Consistent (if we keep increasing the sample size, we eventually obtain accurate estimates)\n",
    "\n",
    "**Disadvantages of MOM**\n",
    "\n",
    "* Often biased (more on biasness later today)\n",
    "* Sometimes gives estimates outside the parameter space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "* The method of maximum likelihood we looked at yesterday is arguably the most popular technique for deriving estimators.\n",
    "\n",
    "* Recall that if $X_1, \\dots, X_n$ are i.i.d. (independently, identically distributed) sample from a population with PDF or PMF $f(x | \\theta_1, \\dots, \\theta_k)$, the likelihood function is defined by\n",
    "\n",
    "$$L(\\theta | \\textbf{x}) = L(\\theta_1, \\dots, \\theta_k | x_1, \\dots, x_n) = \\prod_{i = 1}^n f(x_i | \\theta_1, \\dots, \\theta_k) $$  \n",
    "\n",
    "* Definition: For each sample $\\textbf{x}$, let $\\hat{\\theta}(\\textbf{x})$ be a parameter value at which $L(\\theta | \\textbf{x})$ attains its maximum as a function of $\\theta$, with $\\textbf{x}$ fixed. A **maximum likelihood estimator (MLE)** of the parameter $\\theta$ based on a sample $\\textbf{X}$ is $\\hat{\\theta}(\\textbf{X})$.  \n",
    "\n",
    "* We sometimes write $L(\\theta | \\textbf{x})$ as $L(\\theta)$, and $\\hat{\\theta}(\\textbf{x})$ as $\\hat{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: Bernoulli MLE**\n",
    "\n",
    "Let $X_1, \\dots, X_n$ be i.i.d. $Bernoulli(p)$. Then the likelihood function is\n",
    "\n",
    "$$ L(p | \\textbf{x}) = \\prod_{i = 1}^n p^{x_i} (1 - p)^{1 - x_1} = p^y(1 - p)^{n - y} $$  \n",
    "\n",
    "where $y = \\sum x_i$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While this function is not hard to differentiate, it is much easier to differentiate the log-likelihood  \n",
    "\n",
    "$$ l(p | \\textbf{x}) = log(L(p | \\textbf{x})) = y log(p) + (n - y) log(1 - p) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Differentiate $log(L(p | \\textbf{x}))$ and set it to 0  \n",
    "\n",
    "$$ \\frac{y}{p} - \\frac{n - y}{1 - p} = 0 $$\n",
    "\n",
    "Solve for $p$, and we get,  \n",
    "\n",
    "$$\\hat{p} = \\frac{y}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Estimators\n",
    "\n",
    "In many cases, different methods will lead to different estimators, and in today's class, we will introduce one of the criteria for evaluating estimators.\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "\n",
    "* The **mean squared error (MSE)** of an estimator $\\hat{\\theta}$ of a parameter $\\theta$ is the function of $\\theta$ defined by $E(\\hat{\\theta} - \\theta)^2$.\n",
    "\n",
    "* So basically, the MSE of a particular estimator $\\hat{\\theta}$ is $E(\\hat{\\theta} - \\theta)^2$.  \n",
    "\n",
    "* If we do some math, we can express MSE as\n",
    "\n",
    "$$ E(\\hat{\\theta} - \\theta)^2 = Var(\\hat{\\theta}) + [E(\\hat{\\theta}) - \\theta]^2 = Var(\\hat{\\theta}) + (Bias(\\hat{\\theta}))^2 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bias\n",
    "\n",
    "* The **bias** of a point estimator $\\hat{\\theta}$ is the difference between the expected value of $\\hat{\\theta}$ and $\\theta$.  \n",
    "\n",
    "* That is, $Bias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta$. \n",
    "\n",
    "* If an estimator satisfies $Bias(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta = 0$ for all $\\theta$, it is called an **unbiased estimator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The two components of MSE\n",
    "\n",
    "* $Var(\\hat{\\theta})$, the variability of the estimator (precision)  \n",
    "\n",
    "\n",
    "* $Bias(\\hat{\\theta})$, the bias of the estimator (accuracy)  \n",
    "\n",
    "\n",
    "**To find an estimator with good MSE properties**, we need to find estimators that control both variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Unbiased estimators\n",
    "\n",
    "* Unbiased estimators do a good job of controlling bias  \n",
    "\n",
    "* We have  \n",
    "\n",
    "$$ MSE = E(\\hat{\\theta} - \\theta)^2 = Var(\\hat{\\theta}) $$  \n",
    "\n",
    "* If an estimator is unbiased, its MSE is equal to its variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example: Normal MSE**  \n",
    "\n",
    "Let $X_1, \\dots, X_n$ be i.i.d. $N(\\mu, \\sigma^2)$. The statistics $\\bar{X}$ (sample mean) and $S^2$ (sample variance) are both unbiased estimators since\n",
    "\n",
    "$$ E(\\bar{X}) = \\mu $$  \n",
    "\n",
    "\n",
    "$$ E(S^2) = \\sigma^2 $$  \n",
    "\n",
    "\n",
    "for all $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **MSE**s of these estimators are given by  \n",
    "\n",
    "\n",
    "$$ E(\\bar{X} - \\mu)^2 = Var(\\bar{X}) = \\frac{\\sigma^2}{n} $$  \n",
    "\n",
    "\n",
    "$$ E(S^2 - \\sigma^2)^2 = Var(S^2) = \\frac{2 \\sigma^4}{n - 1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Bias-variance trade-off\n",
    "\n",
    "* Although many unbiased estimators are also reasonable from the standpoint of MSE, keep in mind that controlling bias alone does not guarantee that MSE is controlled.  \n",
    "\n",
    "* In particular, it is sometimes the case that a trade-off occurs between variance and bias in such a way that a small increase in bias can be traded for a larger decrease in variance, resulting in an improved MSE."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
